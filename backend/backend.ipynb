{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "fW5f8VVBssrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0SAy5wqDvIfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/mistralclass.pt /content"
      ],
      "metadata": {
        "id": "dkm7ZDFwwRdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38riuOHBsUXv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "#declaring the constant configurations and model ids\n",
        "\n",
        "#configuration of the mistral  model\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "#configuration of peft lora\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\",\"o_proj\",\"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "#load the model to set-up the backend model to classify dark patterns effectively\n",
        "#also load the Tokenizer and make the parameter add_eos_token=False since only then the model generates the completion\n",
        "\n",
        "def model_load(model_id=model_id, bnb_config=bnb_config,lora_config=lora_config):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=False)\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    peft_model = get_peft_model(model, lora_config)\n",
        "    #model.add_adapter(lora_config, adapter_name=\"adapter\")\n",
        "    model.load_state_dict(torch.load(\"/content/mistralclass.pt\"))\n",
        "    return model,tokenizer\n",
        "\n",
        "#define the alpaca format for the prompt for training and testing purposes\n",
        "def generate_prompt(data_point, return_out=True):\n",
        "    text = 'Given Input that represents text from a webpage, write the correct option for the multiple choice question.\\n\\n'\n",
        "    text += f'### Input:\\n{data_point[\"input\"]}\\nChoose which of the following dark patterns it falls under:\\n' \\\n",
        "             '(A) Urgency\\n(B) Scarcity\\n(C) Misdirection\\n(D) Social Proof\\n(E) Obstruction\\n(F) Sneaking\\n(G) Forced Action\\n(H) Not a Dark Pattern\\n\\n'\n",
        "    text += f'### Answer:\\n{data_point[\"output\"] if return_out else \"\"}'\n",
        "\n",
        "    return text\n",
        "\n",
        "#define function to generate output completions by the model\n",
        "def get_completion_2(query: str, model, tokenizer) -> str:\n",
        "   device = \"cuda:0\"\n",
        "\n",
        "   prompt = generate_prompt(query, return_out=False)\n",
        "\n",
        "   encodeds = tokenizer(prompt, return_tensors=\"pt\", )\n",
        "\n",
        "   model_inputs = encodeds.to(device)\n",
        "\n",
        "   generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=True)\n",
        "   decoded = tokenizer.batch_decode(generated_ids)\n",
        "   return (decoded[0][327+len(query[\"input\"])])\n",
        "\n",
        "#result = get_completion_2(query=d, model=model, tokenizer=tokenizer)\n",
        "#print(result)\n",
        "\n",
        "#include batch size and in colab add parameter for evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model,tokenizer=model_load()"
      ],
      "metadata": {
        "id": "sBvCFJFRsnZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d={\"input\":\"Only 10 left in stock\",\"output\":\"B\"}"
      ],
      "metadata": {
        "id": "PQVNMXpRdMYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Choose which of the following dark patterns it falls under:\n",
        "(A) Urgency\n",
        "(B) Scarcity\n",
        "(C) Misdirection\n",
        "(D) Social Proof\n",
        "(E) Obstruction\n",
        "(F) Sneaking\n",
        "(G) Forced Action\n",
        "(H) Not a Dark Pattern\n",
        "\n",
        "### Answer:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U4roWf4fdiOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = get_completion_2(query=d, model=model, tokenizer=tokenizer)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UXWQGG7dRgn",
        "outputId": "af650e32-e98a-4db9-d9a2-332cef10cfc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "id": "aT-dJJ0B8n96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install views"
      ],
      "metadata": {
        "id": "Fmq3UcwgOYui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask_cors"
      ],
      "metadata": {
        "id": "SGZvbqFyU6zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask --upgrade\n"
      ],
      "metadata": {
        "id": "GBwAh1NMVDbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask\n",
        "\n",
        "from flask import jsonify\n",
        "from flask import request\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "#sys.path.append(os.path.abspath(\"/Users/srivarshinees/dpbh_hackathon/dpbh/darkpatterndetect\"))\n",
        "#from darkpatterndetect.model import model_load, generate_prompt, get_completion_2\n",
        "from flask_cors import CORS\n",
        "#from model import model_load, generate_prompt, get_completion_2\n",
        "port_no=5000\n",
        "ngrok.set_auth_token(\"2bcJ2XNmvunBkFkck5HGNioPBQ2_6DPFz5EpWVfRzEAkQkqJt\")\n",
        "public_url=ngrok.connect(port_no).public_url\n",
        "print(public_url)\n",
        "\n",
        "\n",
        "app=Flask(__name__)\n",
        "\n",
        "\n",
        "CORS(app)\n",
        "\n",
        "#!pip install -U flask-cors\n",
        "#give above command in the host gc\n",
        "\n",
        "\n",
        "#app.add_url_rule('/',view_func=view.main)\n",
        "@app.route(\"/\",methods=['GET','POST'])\n",
        "def main():\n",
        "    #return \"Dark Patterns Buster Hackathon\"\n",
        "    #output=[]\n",
        "    #data=[]\n",
        "    #if request.method== 'POST':\n",
        "      #return \"Dark Patterns Buster Hackathon\"\n",
        "      #data = request.get_json().get('text')\n",
        "      #return \"DArk\n",
        "\n",
        "      #output = []\n",
        "      #data = request.get_json().get('text')\n",
        "      #return \"DArk\"\n",
        "      #check if the entire json file can be sent in as input query and find the format of answer in that case\n",
        "   if request.method=='GET':\n",
        "    return jsonify('{ \\'result\\': ' + str(\"got\") + ' }')\n",
        "   if request.method == 'POST':\n",
        "\n",
        "      output = []\n",
        "      output.append('None')\n",
        "      data = request.get_json().get('token')\n",
        "      for token in data:\n",
        "          leng=len(token)\n",
        "          token_={\"input\":token}\n",
        "          print(token_)\n",
        "          result = get_completion_2(model,query=token_,tokenizer=tokenizer)\n",
        "          print(result)\n",
        "          output.append(result)\n",
        "\n",
        "      dark = [data[i] for i in range(len(output)) if output[i] != 'H']\n",
        "      for d in dark:\n",
        "          print(d)\n",
        "\n",
        "      print(len(dark))\n",
        "\n",
        "\n",
        "      message = '{ \\'result\\': ' + str(output) + ' }'\n",
        "      print(message)\n",
        "\n",
        "\n",
        "\n",
        "      json = jsonify(message)\n",
        "      return json\n",
        "\n",
        "   else:\n",
        "      print(\"Neither GET or POST\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "metadata": {
        "id": "kAL5Dwhp9GJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QdKGHz-qZl1u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}